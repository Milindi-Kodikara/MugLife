{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Da Hell?! "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6eeca1d9528e7fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 0 : : Set up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b58d39f21a950f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from client import client\n",
    "import pre_processing\n",
    "import utils\n",
    "import visualiser\n",
    "\n",
    "from praw.models import MoreComments\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d536d6c40ffe65a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokeniser = TweetTokenizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# add punctuation to stopwords list\n",
    "stop_words = stopwords.words('english') + list(string.punctuation) + ['rt', 'via', '...', 'â€¦', '\"', \"'\", '`', '-', '..']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c661a6147ac4bee0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "collected_posts = []\n",
    "unprocessed_token_lists = []\n",
    "processed_token_lists = []\n",
    "\n",
    "posts_df = pd.DataFrame(columns=['title', 'utc_date', 'formatted_date', 'desc', 'author', 'rating','num_comments', 'unprocessed_tokens', 'processed_tokens'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76076a2f3e72c9a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "collect_data = os.environ[\"COLLECT-DATA\"]\n",
    "data_filepath = os.environ[\"DATA-FILEPATH\"]\n",
    "\n",
    "collect_data = True if collect_data == \"True\" else False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e97c1b6eeafe85",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 1 : : Data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f3209c53ab1a51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data collection from Reddit\n",
    "if collect_data:\n",
    "    subreddit_names = 'tea+coffee+TeaPorn+pourover'\n",
    "    \n",
    "    reddit_client = client()\n",
    "    subreddit = reddit_client.subreddit(subreddit_names)\n",
    "    collected_posts = [*subreddit.top(limit=1)] "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8a23928d30be08",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2 : : Pre-processing and Exploration\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4542f44696aff292"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create dataframe containing reddit post details, unprocessed and pre-processed token lists\n",
    "if collect_data:    \n",
    "    for post in collected_posts:\n",
    "        post_title = post.title\n",
    "        post_description = post.selftext\n",
    "        post_title_description = post_title + \" \" + post_description\n",
    "        post_date = pd.to_datetime(datetime.fromtimestamp(post.created_utc).strftime(\"%d/%m/%Y\"), dayfirst=True)\n",
    "        \n",
    "        unprocessed_tokens = tokeniser.tokenize(post_title_description)\n",
    "        unprocessed_token_lists.append(unprocessed_tokens)\n",
    "        \n",
    "        processed_tokens = pre_processing.process(post_title_description, tokeniser, stemmer, stop_words, True)\n",
    "        # text, tokeniser, stop_words\n",
    "        processed_token_lists.append(processed_tokens)\n",
    "        \n",
    "        if post.author is None:\n",
    "            post_author = 'None'\n",
    "        else:\n",
    "            post_author = post.author.name\n",
    "            \n",
    "        for comment in post.comments:\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "    \n",
    "            comment_text = comment.body if comment.body is None else ''\n",
    "            \n",
    "            unprocessed_comment_tokens = tokeniser.tokenize(comment_text)\n",
    "            unprocessed_tokens = unprocessed_tokens + unprocessed_comment_tokens\n",
    "            unprocessed_token_lists.append(unprocessed_comment_tokens)\n",
    "            \n",
    "            processed_comment_tokens = pre_processing.process(comment_text, tokeniser, stemmer, stop_words, False)\n",
    "            processed_tokens = processed_tokens + processed_comment_tokens\n",
    "            processed_token_lists.append(processed_comment_tokens)\n",
    "            \n",
    "        posts_df.loc[len(posts_df.index)] = [post_title, post.created_utc, post_date, post_description, post_author, post.upvote_ratio, post.num_comments, unprocessed_tokens, processed_tokens]\n",
    "    \n",
    "    # Read old data file if it exists to append new data collected, if not save new file\n",
    "    old_posts_df = pd.DataFrame(columns=['title', 'utc_date', 'formatted_date', 'desc', 'author', 'rating','num_comments', 'unprocessed_tokens', 'processed_tokens'])\n",
    "    if os.path.isfile(data_filepath):\n",
    "        old_posts_df = pd.read_csv(data_filepath, header=0)\n",
    "\n",
    "        posts_df = pd.concat([old_posts_df, posts_df], ignore_index=True)\n",
    "    \n",
    "    posts_df.to_csv(data_filepath, index=False, header=True)\n",
    "\n",
    "len(posts_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa8b2adcad14d37",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "if not collect_data: \n",
    "    posts_df = pd.read_csv(data_filepath)\n",
    "    unprocessed_token_lists = posts_df.unprocessed_tokens.apply(lambda s: list(ast.literal_eval(s)))\n",
    "    processed_token_lists = posts_df.processed_tokens.apply(lambda s: list(ast.literal_eval(s)))\n",
    "\n",
    "posts_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f795135f0ac1ea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_num_posts = len(posts_df)\n",
    "print(f'Total number of posts: {total_num_posts}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a04907dd8dea82",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_num_comments = posts_df['num_comments'].sum()\n",
    "print(f'Total number of comments: {total_num_comments}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bd57e90959306",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_data_items = total_num_posts + total_num_comments\n",
    "print(f'Total data items: {total_data_items}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12156a65e2353f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flatted_unprocessed_token_list = [element for innerList in unprocessed_token_lists for element in innerList]   \n",
    "\n",
    "visualiser.compute_term_freq(flatted_unprocessed_token_list, True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bea5ce626441af0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processed_token_lists = [element for innerList in processed_token_lists for element in innerList]   \n",
    "\n",
    "visualiser.compute_term_freq(processed_token_lists, True, utils.red)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8580700a552f450",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3 : : Method\n",
    "\n",
    "Methods explored:\n",
    "1. Sentiment analysis\n",
    "2. Topic Modelling\n",
    "3. TBA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfdc8bad9f00902"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5572104d860fb4d6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 4 : : Analysis\n",
    "\n",
    "Questions to explore:\n",
    "1. Which is the superior beverage?\n",
    "2. What are the most talked topics?\n",
    "3. Which parts of the world favour which bev? What are their feelings and opinions?\n",
    "4. Since we're in Melbourne, maybe a special look into Melbourne?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c331e1ed2ad759f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
