{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Da Hell?! "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6eeca1d9528e7fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 0 : : Set up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b58d39f21a950f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from client import client\n",
    "import pre_processing\n",
    "import utils\n",
    "import visualiser\n",
    "\n",
    "from praw.models import MoreComments\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d536d6c40ffe65a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subreddit_names = 'tea+coffee+TeaPorn+pourover'\n",
    "\n",
    "reddit_client = client()\n",
    "subreddit = reddit_client.subreddit(subreddit_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8a23928d30be08",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 1 : : Data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f3209c53ab1a51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_top_posts = [*subreddit.top(limit=None)] \n",
    "posts_df = pd.DataFrame(columns=['title', 'utc_date', 'formatted_date', 'desc', 'author', 'rating','num_comments', 'unprocessed_tokens', 'processed_tokens'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76076a2f3e72c9a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokeniser = TweetTokenizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# add punctuation to stopwords list\n",
    "stop_words = stopwords.words('english') + list(string.punctuation) + ['rt', 'via', '...', 'â€¦', '\"', \"'\", '`', '-', '..']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c661a6147ac4bee0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2 : : Pre-processing and Exploration\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4542f44696aff292"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create dataframe containing reddit post details, unprocessed and pre-processed token lists\n",
    "unprocessed_token_lists = []\n",
    "processed_token_lists = []\n",
    "\n",
    "for post in combined_top_posts:\n",
    "    post_title = post.title\n",
    "    post_description = post.selftext\n",
    "    post_title_description = post_title + \" \" + post_description\n",
    "    post_date = pd.to_datetime(datetime.fromtimestamp(post.created_utc).strftime(\"%d/%m/%Y\"), dayfirst=True)\n",
    "    \n",
    "    unprocessed_tokens = tokeniser.tokenize(post_title_description)\n",
    "    unprocessed_token_lists.append(unprocessed_tokens)\n",
    "    \n",
    "    processed_tokens = pre_processing.process(post_title_description, tokeniser, stemmer, stop_words, True)\n",
    "    # text, tokeniser, stop_words\n",
    "    processed_token_lists.append(processed_tokens)\n",
    "    \n",
    "    if post.author is None:\n",
    "        post_author = 'None'\n",
    "    else:\n",
    "        post_author = post.author.name\n",
    "        \n",
    "    for comment in post.comments:\n",
    "        if isinstance(comment, MoreComments):\n",
    "            continue\n",
    "\n",
    "        comment_text = comment.body if comment.body is None else ''\n",
    "        \n",
    "        unprocessed_comment_tokens = tokeniser.tokenize(comment_text)\n",
    "        unprocessed_tokens = unprocessed_tokens + unprocessed_comment_tokens\n",
    "        unprocessed_token_lists.append(unprocessed_comment_tokens)\n",
    "        \n",
    "        processed_comment_tokens = pre_processing.process(comment_text, tokeniser, stemmer, stop_words, False)\n",
    "        processed_tokens = processed_tokens + processed_comment_tokens\n",
    "        processed_token_lists.append(processed_comment_tokens)\n",
    "        \n",
    "    posts_df.loc[len(posts_df.index)] = [post_title, post.created_utc, post_date, post_description, post_author, post.upvote_ratio, post.num_comments, unprocessed_tokens, processed_tokens]\n",
    "\n",
    "len(posts_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa8b2adcad14d37",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "posts_df.to_csv('data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfba57ca921c23aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('data.csv')\n",
    "posts_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f795135f0ac1ea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_num_posts = len(posts_df)\n",
    "print(f'Total number of posts: {total_num_posts}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a04907dd8dea82",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_num_comments = posts_df['num_comments'].sum()\n",
    "print(f'Total number of comments: {total_num_comments}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bd57e90959306",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_data_items = total_num_posts + total_num_comments\n",
    "print(f'Total data items: {total_data_items}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12156a65e2353f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flatted_unprocessed_token_list = [element for innerList in unprocessed_token_lists for element in innerList]   \n",
    "\n",
    "visualiser.compute_term_freq(flatted_unprocessed_token_list, True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bea5ce626441af0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processed_token_lists = [element for innerList in processed_token_lists for element in innerList]   \n",
    "\n",
    "visualiser.compute_term_freq(processed_token_lists, True, utils.red)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8580700a552f450",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3 : : Method\n",
    "\n",
    "Methods explored:\n",
    "1. Sentiment analysis\n",
    "2. Topic Modelling\n",
    "3. TBA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfdc8bad9f00902"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5572104d860fb4d6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 4 : : Analysis\n",
    "\n",
    "Questions to explore:\n",
    "1. Which is the superior beverage?\n",
    "2. What are the most talked topics?\n",
    "3. Which parts of the world favour which bev? What are their feelings and opinions?\n",
    "4. Since we're in Melbourne, maybe a special look into Melbourne?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c331e1ed2ad759f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
