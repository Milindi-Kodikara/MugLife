{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### \\#MugLife"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6eeca1d9528e7fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 0 : : Set up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b58d39f21a950f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "from client import client\n",
    "import pre_processing\n",
    "import utils\n",
    "import visualiser\n",
    "import method\n",
    "\n",
    "from praw.models import MoreComments\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.lda_model\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "install(\"python-louvain\")\n",
    "load_dotenv()\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d536d6c40ffe65a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokeniser = TweetTokenizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# add punctuation to stopwords list\n",
    "stop_words = stopwords.words('english') + list(string.punctuation) + ['rt', 'via', '...', 'â€¦', '\"', \"'\", '`', '-', '..']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c661a6147ac4bee0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "collected_posts = []\n",
    "unprocessed_token_lists = []\n",
    "processed_token_lists = []\n",
    "\n",
    "posts_df = pd.DataFrame(columns=['social_media_id', 'title', 'utc_date', 'formatted_date', 'desc', 'author', 'rating','num_comments', 'unprocessed_tokens', 'processed_tokens'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76076a2f3e72c9a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "social_media_id = os.environ[\"SOCIAL-MEDIA-ID\"]\n",
    "social_media_id = social_media_id.lower()\n",
    "\n",
    "collect_data_env = os.environ[\"COLLECT-DATA\"]  \n",
    "\n",
    "data_folder_path = os.environ[\"DATA-FOLDER-PATH\"]\n",
    "\n",
    "collect_data = True if collect_data_env == \"True\" else False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e97c1b6eeafe85",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 1 : : Data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f3209c53ab1a51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data collection from Reddit\n",
    "data_sample_filepath = f'{data_folder_path}/data.csv'\n",
    "\n",
    "if collect_data:\n",
    "    if social_media_id == 'reddit':\n",
    "        subreddit_names = 'tea+coffee+TeaPorn+pourover'\n",
    "        \n",
    "        reddit_client = client()\n",
    "        subreddit = reddit_client.subreddit(subreddit_names)\n",
    "        collected_posts = [*subreddit.top(limit=None)] "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8a23928d30be08",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2 : : Pre-processing and Exploration\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4542f44696aff292"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create dataframe containing reddit post details, unprocessed and pre-processed token lists\n",
    "# This bit extracts the data from reddit and saves it to the data file \n",
    "if collect_data:\n",
    "    reply_graph = nx.DiGraph()\n",
    "    reply_graph_filepath = f'{data_folder_path}/{social_media_id}_reply_graph.graphml'\n",
    "    \n",
    "    # track the ids of post and comments for the reply graph\n",
    "    post_comment_ids = dict()\n",
    "\n",
    "    for post in collected_posts:\n",
    "        post_id = post.name\n",
    "\n",
    "        post_title = post.title\n",
    "        post_description = post.selftext\n",
    "        post_title_description = post_title + \" \" + post_description\n",
    "        post_date = pd.to_datetime(datetime.fromtimestamp(post.created_utc).strftime(\"%d/%m/%Y\"), format=\"%d/%m/%Y\")\n",
    "\n",
    "        unprocessed_tokens = tokeniser.tokenize(post_title_description)\n",
    "        unprocessed_token_lists.append(unprocessed_tokens)\n",
    "\n",
    "        processed_tokens = pre_processing.process(post_title_description, tokeniser, stemmer, stop_words, True)\n",
    "        # text, tokeniser, stop_words\n",
    "        processed_token_lists.append(processed_tokens)\n",
    "\n",
    "        if post.author is None:\n",
    "            post_author = 'None'\n",
    "        else:\n",
    "            post_author = post.author.name\n",
    "\n",
    "        reply_graph = method.update_reply_graph_node(reply_graph, post_author)\n",
    "        # Add the post id and the author to list of ids  \n",
    "        post_comment_ids[post_id] = {post_id: post_author}\n",
    "\n",
    "        post.comments.replace_more(limit=None)\n",
    "        for comment in post.comments:\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "\n",
    "            comment_text = comment.body if comment.body is None else ''\n",
    "\n",
    "            unprocessed_comment_tokens = tokeniser.tokenize(comment_text)\n",
    "            unprocessed_tokens = unprocessed_tokens + unprocessed_comment_tokens\n",
    "            unprocessed_token_lists.append(unprocessed_comment_tokens)\n",
    "\n",
    "            processed_comment_tokens = pre_processing.process(comment_text, tokeniser, stemmer, stop_words, False)\n",
    "            processed_tokens = processed_tokens + processed_comment_tokens\n",
    "            processed_token_lists.append(processed_comment_tokens)\n",
    "\n",
    "            # Check if comment author exists\n",
    "            comment_name = comment.name\n",
    "            comment_author = comment.author\n",
    "            if comment_author is not None and comment_author.name != 'ExternalUserError':\n",
    "                comment_author_name = comment_author.name\n",
    "\n",
    "                # Link the comment and comment author to the post id\n",
    "                post_comment_ids[post_id].update({comment_name: comment_author_name})\n",
    "\n",
    "                # Check whether parent comment is in the ids list  \n",
    "                # If not, then parent comment has been deleted\n",
    "                comment_parent_id = comment.parent_id\n",
    "                if comment_parent_id in post_comment_ids[post_id]:\n",
    "                    reply_graph = method.update_reply_graph_edge(reply_graph, comment_author_name, post_comment_ids,\n",
    "                                                                 post_id, comment_parent_id)\n",
    "\n",
    "        posts_df.loc[len(posts_df.index)] = [social_media_id, post_title, post.created_utc, post_date, post_description,\n",
    "                                             post_author, post.upvote_ratio, post.num_comments, unprocessed_tokens,\n",
    "                                             processed_tokens]\n",
    "        \n",
    "    # Save reply graph\n",
    "    nx.readwrite.write_graphml(reply_graph, reply_graph_filepath)\n",
    "    # Read old data file if it exists to append new data collected, if not save new file\n",
    "    old_posts_df = pd.DataFrame(\n",
    "        columns=['social_media_id', 'title', 'utc_date', 'formatted_date', 'desc', 'author', 'rating', 'num_comments',\n",
    "                 'unprocessed_tokens', 'processed_tokens'])\n",
    "\n",
    "    if os.path.isfile(data_sample_filepath):\n",
    "        old_posts_df = pd.read_csv(data_sample_filepath, header=0)\n",
    "\n",
    "        posts_df = pd.concat([old_posts_df, posts_df], ignore_index=True)\n",
    "\n",
    "    posts_df.to_csv(data_sample_filepath, index=False, header=True)\n",
    "\n",
    "len(posts_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa8b2adcad14d37",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "if not collect_data: \n",
    "    posts_df = pd.read_csv(data_sample_filepath)\n",
    "    unprocessed_token_lists = posts_df.unprocessed_tokens.apply(lambda s: list(ast.literal_eval(s)))\n",
    "    posts_df['unprocessed_tokens'] = unprocessed_token_lists\n",
    "    processed_token_lists = posts_df.processed_tokens.apply(lambda s: list(ast.literal_eval(s)))\n",
    "    posts_df['processed_tokens'] = processed_token_lists\n",
    "    \n",
    "    posts_df['formatted_date'] = pd.to_datetime(posts_df['formatted_date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "posts_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f795135f0ac1ea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_num_posts = len(posts_df)\n",
    "print(f'Total number of posts: {total_num_posts}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a04907dd8dea82",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_num_comments = posts_df['num_comments'].sum()\n",
    "print(f'Total number of comments: {total_num_comments}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bd57e90959306",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_data_items = total_num_posts + total_num_comments\n",
    "print(f'Total data items: {total_data_items}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12156a65e2353f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_social_medias = posts_df['social_media_id'].unique()\n",
    "print(f'Social media data was collected from:\\n{df_social_medias}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97607f2fff1513e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flatted_unprocessed_token_list = [element for innerList in unprocessed_token_lists for element in innerList]   \n",
    "\n",
    "visualiser.compute_term_freq(flatted_unprocessed_token_list, True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bea5ce626441af0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processed_token_lists = [element for innerList in processed_token_lists for element in innerList]   \n",
    "\n",
    "visualiser.compute_term_freq(processed_token_lists, True, utils.red)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8580700a552f450",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3 : : Method\n",
    "\n",
    "Methods explored:\n",
    "1. N-grams were explored to gain preliminary understanding of the sentiments in this subreddit\n",
    "2. Sentiment analysis via N-grams, Count and Vader techniques \n",
    "3. Topic modelling via LDA topic model\n",
    "4. Ego-graph\n",
    "5. Reply graph\n",
    "6. Community detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfdc8bad9f00902"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# N-grams\n",
    "top_50_bi_grams =  nltk.collocations.BigramCollocationFinder.from_words(processed_token_lists).ngram_fd.most_common(50)\n",
    "top_50_tri_grams = nltk.collocations.TrigramCollocationFinder.from_words(processed_token_lists).ngram_fd.most_common(50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5572104d860fb4d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "count_sentiment_list = method.sentiment_analysis('Count', posts_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd7702fa822a203e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vader_sentiment_list = method.sentiment_analysis('Vader', posts_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "782a78100108105a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Topic modelling\n",
    "num_topic = 10\n",
    "num_features = 1500\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(processed_token_lists)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topic, max_iter=10, learning_method='online').fit(tf)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8aac3e8b39186f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Graphs and networks\n",
    "\n",
    "# Egonet\n",
    "# get the top author/s from the posts \n",
    "posts_df_by_rating = posts_df.sort_values(['rating', 'num_comments'], ascending=[False, False])\n",
    "posts_df_by_rating_filtered = posts_df_by_rating[posts_df_by_rating['author'] != 'None']\n",
    "subset_top_rated_authors_df = posts_df_by_rating_filtered.head(1)\n",
    "\n",
    "print('------------Ego graph exploration------------\\n')\n",
    "ego_graph_list = []\n",
    "for row in subset_top_rated_authors_df.itertuples():\n",
    "    author_name = row.author   \n",
    "    row_social_media_id = row.social_media_id\n",
    "    \n",
    "    print(utils.yellow_rgb + f'Social media id: {social_media_id}\\n', end='')\n",
    "    print(utils.yellow_rgb + f'Author name: {author_name}\\nAuthor rating: {row.rating}\\nAuthor comments: {row.num_comments}\\n', end='')\n",
    "    \n",
    "    if row_social_media_id == 'reddit':\n",
    "        if not collect_data:\n",
    "            reddit_client = client()\n",
    "        ego = reddit_client.redditor(author_name)\n",
    "        ego_name = ego.name\n",
    "        ego_graph = method.construct_ego_graph(reddit_client, ego, ego_name)\n",
    "        ego_graph_list.append({'ego_graph': ego_graph, 'ego_name': ego_name})\n",
    "        \n",
    "        # Note: print_ego_graph does not depend on the social media used\n",
    "        utils.print_ego_graph(data_folder_path, ego_graph, ego_name)\n",
    "        \n",
    "subset_top_rated_authors_df  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45a3f234fc005b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if 'reddit' in df_social_medias:\n",
    "    reddit_reply_graph_filepath = f'{data_folder_path}/reddit_reply_graph.graphml'\n",
    "    reddit_reply_graph = nx.readwrite.read_graphml(reddit_reply_graph_filepath)\n",
    "    \n",
    "    # Reply graph\n",
    "    print('\\n------------Reply graph exploration------------\\n')\n",
    "    method.compute_reply_graph_stats(reddit_reply_graph, data_folder_path, 'reddit')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82146a5db285996f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if 'reddit' in df_social_medias:\n",
    "    reddit_reply_graph_filepath = f'{data_folder_path}/reddit_reply_graph.graphml'\n",
    "    reddit_reply_graph = nx.readwrite.read_graphml(reddit_reply_graph_filepath)\n",
    "    \n",
    "    # Create community\n",
    "    print('\\n------------Community graph exploration------------\\n')\n",
    "    method.compute_community_stats(reddit_reply_graph, data_folder_path, 'reddit')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6252380de8db2ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 4 : : Analysis\n",
    "\n",
    "Questions to explore:\n",
    "1. Which is the superior beverage?\n",
    "2. What are the most talked topics?\n",
    "3. Which parts of the world favour which bev? What are their feelings and opinions?\n",
    "4. Since we're in Melbourne, maybe a special look into Melbourne?\n",
    "5. Spike in engagement of people with sales and deals; limited time events, world tea/coffee days, variation of engagement with change of season -- Event and correlations \n",
    "6. Origin of tea/ coffee\n",
    "7. Benefits people get from tea/ coffee"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c331e1ed2ad759f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# n-grams\n",
    "top_50_bi_grams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d32b34150867ed64",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "top_50_tri_grams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e1e4ab6b2852da1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Posts per date\n",
    "num_posts_per_date = posts_df.groupby('formatted_date')['title'].count()\n",
    "visualiser.display_time_series_stats(num_posts_per_date, 'count', 'Number of posts per date', 'Dates', 'Number of posts', utils.red)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c20d18e25d32f6d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Posts per author\n",
    "# Displaying authors with only more than 1 post\n",
    "num_posts_per_author = posts_df.groupby('author')['title'].count()\n",
    "\n",
    "num_posts_per_author_ordered = num_posts_per_author.reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "print(f'Posts per author:\\n{num_posts_per_author_ordered.head()}')\n",
    "\n",
    "filtered_df = num_posts_per_author_ordered[num_posts_per_author_ordered['count'] > 5 ]\n",
    "filtered_df = filtered_df[filtered_df['author'] != 'None']\n",
    "\n",
    "num_posts_per_author_y = filtered_df['count']\n",
    "author_x = filtered_df['author']\n",
    "visualiser.generate_bar_chart(author_x, num_posts_per_author_y, utils.red, 'Number of posts per author', 'Author', 'Number of posts')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4147e8a33105eba8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "# Count\n",
    "visualiser.generate_time_series(count_sentiment_list, 'Sentiment based on count', 'date', 'sentiment', 'Date', 'Count sentiment', utils.green)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b17a7d2b10085ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Vader\n",
    "visualiser.generate_time_series(vader_sentiment_list, 'Sentiment based on vader', 'date', 'sentiment', 'Date', 'Veder sentiment', utils.green)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3cbe97170db62d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Topic modelling\n",
    "max_word_count_to_display = 15\n",
    "visualiser.display_topics(lda_model, tf_feature_names, max_word_count_to_display)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab40ed9fbbefbca5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pyLDAvis\n",
    "panel = pyLDAvis.lda_model.prepare(lda_model, tf, tf_vectorizer, mds='tsne')\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(panel)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d11932142bd1fc3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# wordcloud\n",
    "visualiser.display_word_cloud(lda_model, tf_feature_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5fe75b215f13209",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display the ego graphs for the top users\n",
    "\n",
    "for item in ego_graph_list:\n",
    "    ego_graph = item.get('ego_graph')\n",
    "    ego_name = item.get('ego_name')\n",
    "    print(f'Ego name: {ego_name}\\n\\n')\n",
    "    visualiser.display_networkx_graph(ego_graph, f'Ego graph for {ego_name}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "292a1646c4199682",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display reply graph\n",
    "\n",
    "if 'reddit' in df_social_medias:\n",
    "    reply_graph_filepath = f'{data_folder_path}/{social_media_id}_reply_graph.graphml'\n",
    "    reply_graph = nx.readwrite.read_graphml(reply_graph_filepath)\n",
    "    visualiser.display_networkx_graph(reply_graph, 'Reddit reply graph')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efd6610380ddfccb",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
